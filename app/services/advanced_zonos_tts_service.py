"""
Advanced ZONOS TTS Service - í•œêµ­ì–´ ìµœì í™” ì™„ì „ êµ¬í˜„
=======================================================

ZONOS TTS ëª¨ë¸ì˜ ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ì— ìµœì í™”ëœ 
ê³ ê¸‰ TTS ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- í•œêµ­ì–´ ì§ì ‘ ì§€ì› (ko ì–¸ì–´ ì½”ë“œ)
- ì •êµí•œ ê°ì • ì œì–´ (7ê°€ì§€ í”„ë¦¬ì…‹ + ì»¤ìŠ¤í…€)
- ê³ í’ˆì§ˆ Voice Cloning
- ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
"""

import os
import sys
import tempfile
import torch
import torchaudio
import numpy as np
from pathlib import Path
import soundfile as sf
from typing import List, Optional, Dict, Union, Tuple
import traceback
import time
import hashlib
import json
from dataclasses import dataclass

# ZONOS ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
zonos_path = "/home/hsc0125/Hing_tts/models/Zonos"
if zonos_path not in sys.path:
    sys.path.insert(0, zonos_path)

try:
    from zonos.model import Zonos
    from zonos.conditioning import make_cond_dict
    from zonos.utils import DEFAULT_DEVICE
    ZONOS_AVAILABLE = True
    print("âœ… Advanced ZONOS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Advanced ZONOS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    ZONOS_AVAILABLE = False


@dataclass
class AudioMetadata:
    """ìƒì„±ëœ ì˜¤ë””ì˜¤ì˜ ë©”íƒ€ë°ì´í„°"""
    duration: float
    sample_rate: int
    file_size: int
    generation_time: float
    model_config: dict


class SpeakerEmbeddingCache:
    """Speaker embedding ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_size: int = 100):
        self.cache = {}
        self.max_size = max_size
        self.access_times = {}
    
    def _get_key(self, audio_path: str) -> str:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(audio_path.encode()).hexdigest()
    
    def get(self, audio_path: str) -> Optional[torch.Tensor]:
        """ìºì‹œì—ì„œ speaker embedding ì¡°íšŒ"""
        key = self._get_key(audio_path)
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def put(self, audio_path: str, embedding: torch.Tensor):
        """Speaker embeddingì„ ìºì‹œì— ì €ì¥"""
        key = self._get_key(audio_path)
        
        # ìºì‹œ í¬ê¸° ì´ˆê³¼ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = embedding.clone()
        self.access_times[key] = time.time()
        print(f"ğŸ”„ Speaker embedding ìºì‹œë¨: {os.path.basename(audio_path)}")
    
    def clear(self):
        """ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        self.cache.clear()
        self.access_times.clear()
        print("ğŸ—‘ï¸ Speaker embedding ìºì‹œ ì´ˆê¸°í™”ë¨")


class AdvancedZonosTTSService:
    """Advanced ZONOS TTS Service - í•œêµ­ì–´ ìµœì í™”"""
    
    # í•œêµ­ì–´ ìµœì í™” ê°ì • í”„ë¦¬ì…‹
    EMOTION_PRESETS = {
        "neutral": [0.15, 0.05, 0.05, 0.05, 0.05, 0.05, 0.2, 0.4],    # ì¤‘ì„± - ë‰´ìŠ¤/ë‚´ë ˆì´ì…˜
        "happy": [0.6, 0.05, 0.05, 0.05, 0.1, 0.05, 0.1, 0.05],       # ê¸°ì¨ - ê´‘ê³ /í™œë°œí•œ ìŒì„±
        "sad": [0.05, 0.5, 0.05, 0.05, 0.05, 0.05, 0.25, 0.05],       # ìŠ¬í”” - ê°ì„±ì  í‘œí˜„
        "angry": [0.05, 0.05, 0.05, 0.05, 0.05, 0.6, 0.2, 0.05],      # ë¶„ë…¸ - ê°•í•œ í‘œí˜„
        "surprised": [0.05, 0.05, 0.05, 0.05, 0.6, 0.05, 0.2, 0.05],  # ë†€ë¼ì›€ - ë¦¬ì•¡ì…˜
        "calm": [0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.55],       # ì°¨ë¶„í•¨ - ëª…ìƒ/ì§„ì¤‘
        "expressive": [0.25, 0.1, 0.05, 0.1, 0.15, 0.1, 0.2, 0.05]    # í‘œí˜„ë ¥ - ì—°ê¸°/ë“œë¼ë§ˆ
    }
    
    # í•œêµ­ì–´ ìµœì í™” ê¸°ë³¸ ì„¤ì •
    KOREAN_OPTIMAL_CONFIG = {
        "language": "ko",              # í•œêµ­ì–´ ì§ì ‘ ì§€ì›
        "fmax": 22050.0,              # Voice cloning ê¶Œì¥ê°’  
        "pitch_std": 30.0,            # í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ì–µì–‘
        "speaking_rate": 13.0,        # ì ë‹¹í•œ ë°œí™” ì†ë„
        "cfg_scale": 2.5,             # ì•ˆì •ì  ìƒì„± í’ˆì§ˆ
        "max_new_tokens_per_char": 8, # í…ìŠ¤íŠ¸ ê¸¸ì´ ëŒ€ë¹„ í† í° ìˆ˜
    }
    
    def __init__(self, model_type: str = "transformer"):
        """
        Advanced ZONOS TTS ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_type: "transformer" ë˜ëŠ” "hybrid" (hybridëŠ” ë” ë†’ì€ í’ˆì§ˆ, ë” ë§ì€ ë¦¬ì†ŒìŠ¤ í•„ìš”)
        """
        self.model_type = model_type
        self.model_path = "/home/hsc0125/Hing_tts/models/Zonos"
        self.audio_samples_path = "/home/hsc0125/Hing_tts/models/audio_data"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.korean_voices = {}
        self.embedding_cache = SpeakerEmbeddingCache(max_size=50)
        
        print(f"ğŸ™ï¸ Advanced ZONOS TTS ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘... ë””ë°”ì´ìŠ¤: {self.device}, ëª¨ë¸: {model_type}")
        self._initialize_model()
        self._load_korean_voices()
    
    def _initialize_model(self):
        """ZONOS TTS ëª¨ë¸ ì´ˆê¸°í™”"""
        if not ZONOS_AVAILABLE:
            print("âš ï¸ ZONOS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.model = None
            return
            
        try:
            print(f"ğŸ“ ZONOS {self.model_type} ëª¨ë¸ ë¡œë”©...")
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ repo ID ì„ íƒ
            repo_id = f"Zyphra/Zonos-v0.1-{self.model_type}"
            
            # ZONOS ëª¨ë¸ ë¡œë“œ (Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)
            self.model = Zonos.from_pretrained(repo_id, device=self.device)
            self.model.requires_grad_(False).eval()
            
            print(f"âœ… Advanced ZONOS {self.model_type} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"ğŸ¯ í•œêµ­ì–´ ìµœì í™” ì„¤ì • ì ìš©ë¨")
            
        except Exception as e:
            print(f"âŒ Advanced ZONOS ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("âš ï¸ ë”ë¯¸ ëª¨ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            traceback.print_exc()
            self.model = None
    
    def _load_korean_voices(self):
        """í•œêµ­ì–´ ìŒì„± ìƒ˜í”Œë“¤ì„ ë¡œë“œ"""
        try:
            audio_data_path = Path(self.audio_samples_path)
            print(f"ğŸ” ì˜¤ë””ì˜¤ ê²½ë¡œ í™•ì¸: {audio_data_path}")
            
            if not audio_data_path.exists():
                print(f"âš ï¸ ì˜¤ë””ì˜¤ ìƒ˜í”Œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.audio_samples_path}")
                audio_data_path.mkdir(parents=True, exist_ok=True)
                print("ğŸ“ ì˜¤ë””ì˜¤ í´ë” ìƒì„±ë¨")
                return
            
            # ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ íŒŒì¼ ìš°ì„  ìˆœìœ„
            audio_extensions = ['*.wav', '*.flac', '*.mp3', '*.ogg']
            audio_files = []
            
            for ext in audio_extensions:
                found_files = list(audio_data_path.glob(ext))
                audio_files.extend(found_files)
            
            print(f"ğŸ” ë°œê²¬ëœ ì˜¤ë””ì˜¤ íŒŒì¼: {len(audio_files)}ê°œ")
            for f in audio_files:
                print(f"  - {f.name}")
            
            if not audio_files:
                print("âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ìŒì„±ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                return
            
            # í•œêµ­ì–´ í™”ìëª… ë§¤í•‘
            korean_speaker_names = [
                "í•œêµ­ì—¬ì„±1", "í•œêµ­ì—¬ì„±2", "í•œêµ­ì—¬ì„±3", "í•œêµ­ì—¬ì„±4",
                "í•œêµ­ë‚¨ì„±1", "í•œêµ­ë‚¨ì„±2", "í•œêµ­ë‚¨ì„±3", "í•œêµ­ë‚¨ì„±4"
            ]
            
            for i, audio_file in enumerate(audio_files[:len(korean_speaker_names)]):
                speaker_name = korean_speaker_names[i]
                self.korean_voices[speaker_name] = str(audio_file)
                print(f"ğŸ¤ Advanced ZONOS í•œêµ­ì–´ ìŒì„± ë“±ë¡: {speaker_name} -> {audio_file.name}")
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            if audio_files:
                self.korean_voices["default"] = str(audio_files[0])
                print(f"âœ… Advanced ZONOS ê¸°ë³¸ ìŒì„± ì„¤ì •: {audio_files[0].name}")
                
        except Exception as e:
            print(f"âŒ Advanced ZONOS í•œêµ­ì–´ ìŒì„± ìƒ˜í”Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
    
    def _create_speaker_embedding(self, audio_path: str) -> Optional[torch.Tensor]:
        """Speaker embedding ìƒì„± (ìºì‹± ì§€ì›)"""
        if not self.model:
            return None
            
        # ìºì‹œì—ì„œ í™•ì¸
        cached_embedding = self.embedding_cache.get(audio_path)
        if cached_embedding is not None:
            print(f"ğŸ”„ ìºì‹œëœ Speaker embedding ì‚¬ìš©: {os.path.basename(audio_path)}")
            return cached_embedding
        
        try:
            print(f"ğŸ¤ Speaker embedding ìƒì„± ì¤‘: {os.path.basename(audio_path)}")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
            wav, sr = torchaudio.load(audio_path)
            
            # ëª¨ë…¸ ë³€í™˜
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)
            
            # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (ZONOS ê¶Œì¥)
            if sr != 16000:
                resampler = torchaudio.transforms.Resample(sr, 16000)
                wav = resampler(wav)
            
            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 30ì´ˆ)
            max_samples = 16000 * 30
            if wav.shape[1] > max_samples:
                wav = wav[:, :max_samples]
                print(f"  ğŸ“ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¡°ì •: {wav.shape[1] / 16000:.1f}ì´ˆ")
            
            # Speaker embedding ìƒì„±
            embedding = self.model.make_speaker_embedding(wav, 16000)
            
            # ìºì‹œì— ì €ì¥
            self.embedding_cache.put(audio_path, embedding)
            
            print(f"âœ… Speaker embedding ìƒì„± ì™„ë£Œ: {embedding.shape}")
            return embedding
            
        except Exception as e:
            print(f"âŒ Speaker embedding ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def list_korean_voices(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í•œêµ­ì–´ ìŒì„± ëª©ë¡ ë°˜í™˜"""
        return list(self.korean_voices.keys())
    
    def list_emotion_presets(self) -> Dict[str, List[float]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì • í”„ë¦¬ì…‹ ëª©ë¡ ë°˜í™˜"""
        return self.EMOTION_PRESETS.copy()
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "device": self.device,
            "korean_voices_count": len(self.korean_voices),
            "emotion_presets_count": len(self.EMOTION_PRESETS),
            "cache_size": len(self.embedding_cache.cache),
            "optimal_config": self.KOREAN_OPTIMAL_CONFIG,
            "is_loaded": self.model is not None
        }
    
    def estimate_generation_time(self, text: str) -> Dict:
        """ìŒì„± ìƒì„± ì˜ˆìƒ ì‹œê°„ ê³„ì‚°"""
        text_length = len(text)
        char_per_second = 15  # í•œêµ­ì–´ ê¸°ì¤€ í‰ê· 
        estimated_audio_duration = text_length / char_per_second
        
        # GPU ì„±ëŠ¥ì— ë”°ë¥¸ ìƒì„± ì‹œê°„ (Real-time factor ~2x)
        generation_time = estimated_audio_duration / 2.0
        
        return {
            "text_length": text_length,
            "estimated_audio_duration": estimated_audio_duration,
            "estimated_generation_time": generation_time,
            "real_time_factor": 2.0
        }
    
    def generate_speech(
        self, 
        text: str, 
        emotion: Union[str, List[float]] = "neutral",
        speaker_name: Optional[str] = None,
        cfg_scale: float = 2.5,
        pitch_std: Optional[float] = None,
        speaking_rate: Optional[float] = None,
        fmax: Optional[float] = None
    ) -> Tuple[str, AudioMetadata]:
        """
        ê³ í’ˆì§ˆ í•œêµ­ì–´ ìŒì„± ìƒì„±
        
        Args:
            text: ìƒì„±í•  í…ìŠ¤íŠ¸
            emotion: ê°ì • ("neutral", "happy", "sad", etc.) ë˜ëŠ” ì»¤ìŠ¤í…€ 8ì°¨ì› ë²¡í„°
            speaker_name: í™”ìëª… (Noneì´ë©´ ê¸°ë³¸ í™”ì)
            cfg_scale: ìƒì„± í’ˆì§ˆ ì œì–´ (1.0-5.0, ë†’ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ)
            pitch_std: í”¼ì¹˜ ë³€í™”í­ (ê¸°ë³¸ê°’: 30.0)
            speaking_rate: ë°œí™” ì†ë„ (ê¸°ë³¸ê°’: 13.0)
            fmax: ìµœëŒ€ ì£¼íŒŒìˆ˜ (ê¸°ë³¸ê°’: 22050.0)
            
        Returns:
            (audio_file_path, metadata)
        """
        if not self.model:
            raise Exception("Advanced ZONOS TTS ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        # ì„ì‹œ ì¶œë ¥ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            output_path = tmp_file.name
        
        try:
            print(f"ğŸ¬ Advanced ZONOS TTS ìŒì„± ìƒì„± ì‹œì‘: {text[:50]}...")
            
            # ê°ì • ì„¤ì • ì²˜ë¦¬
            if isinstance(emotion, str):
                if emotion not in self.EMOTION_PRESETS:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ê°ì • '{emotion}', 'neutral'ë¡œ ëŒ€ì²´")
                    emotion = "neutral"
                emotion_vector = self.EMOTION_PRESETS[emotion]
                print(f"ğŸ­ ê°ì • ì„¤ì •: {emotion}")
            else:
                if len(emotion) != 8:
                    raise ValueError("ì»¤ìŠ¤í…€ ê°ì • ë²¡í„°ëŠ” 8ì°¨ì›ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
                emotion_vector = emotion
                print(f"ğŸ­ ì»¤ìŠ¤í…€ ê°ì • ë²¡í„° ì ìš©: {emotion_vector}")
            
            # í™”ì ì„¤ì •
            speaker_embedding = None
            if self.korean_voices:
                voice_path = None
                
                if speaker_name and speaker_name in self.korean_voices:
                    voice_path = self.korean_voices[speaker_name]
                    print(f"ğŸ¤ ì§€ì •ëœ í™”ì: {speaker_name}")
                elif "default" in self.korean_voices:
                    voice_path = self.korean_voices["default"]
                    print(f"ğŸ¤ ê¸°ë³¸ í™”ì ì‚¬ìš©")
                else:
                    voice_path = list(self.korean_voices.values())[0]
                    print(f"ğŸ¤ ì²« ë²ˆì§¸ í™”ì ì‚¬ìš©")
                
                if voice_path:
                    speaker_embedding = self._create_speaker_embedding(voice_path)
            
            # ì„¤ì •ê°’ ìµœì í™” ì ìš©
            config = self.KOREAN_OPTIMAL_CONFIG.copy()
            if pitch_std is not None:
                config["pitch_std"] = pitch_std
            if speaking_rate is not None:
                config["speaking_rate"] = speaking_rate
            if fmax is not None:
                config["fmax"] = fmax
            
            # Conditioning dictionary ìƒì„±
            cond_dict = make_cond_dict(
                text=text,
                language=config["language"],
                speaker=speaker_embedding,
                emotion=emotion_vector,
                fmax=config["fmax"],
                pitch_std=config["pitch_std"],
                speaking_rate=config["speaking_rate"],
                device=self.device
            )
            
            print(f"ğŸ”§ ì„¤ì •ê°’ - í”¼ì¹˜: {config['pitch_std']}, ì†ë„: {config['speaking_rate']}, CFG: {cfg_scale}")
            
            # Conditioning ì¤€ë¹„
            conditioning = self.model.prepare_conditioning(cond_dict)
            
            # ìŒì„± ìƒì„±
            print("ğŸµ Advanced ZONOSë¡œ ê³ í’ˆì§ˆ ìŒì„± ìƒì„± ì¤‘...")
            max_tokens = int(len(text) * config["max_new_tokens_per_char"])
            
            with torch.inference_mode():
                codes = self.model.generate(
                    conditioning,
                    cfg_scale=cfg_scale,
                    max_new_tokens=max_tokens,
                    progress_bar=True,
                    disable_torch_compile=True  # ì•ˆì •ì„± í™•ë³´
                )
            
            # ì˜¤ë””ì˜¤ ë””ì½”ë”©
            print("ğŸ”„ 44.1kHz ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ ë””ì½”ë”© ì¤‘...")
            wavs = self.model.autoencoder.decode(codes).cpu()
            
            # WAV íŒŒì¼ë¡œ ì €ì¥ (ZONOSëŠ” 44.1kHz ë„¤ì´í‹°ë¸Œ)
            sample_rate = self.model.autoencoder.sampling_rate
            torchaudio.save(output_path, wavs[0], sample_rate)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            generation_time = time.time() - start_time
            file_size = os.path.getsize(output_path)
            duration = wavs[0].shape[1] / sample_rate
            
            metadata = AudioMetadata(
                duration=duration,
                sample_rate=sample_rate,
                file_size=file_size,
                generation_time=generation_time,
                model_config={
                    "model_type": self.model_type,
                    "emotion": emotion if isinstance(emotion, str) else "custom",
                    "cfg_scale": cfg_scale,
                    "pitch_std": config["pitch_std"],
                    "speaking_rate": config["speaking_rate"],
                    "fmax": config["fmax"]
                }
            )
            
            print(f"ğŸ’¾ Advanced ZONOS ìŒì„± íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            print(f"ğŸ“Š ìŒì„± ê¸¸ì´: {duration:.2f}ì´ˆ, íŒŒì¼ í¬ê¸°: {file_size/1024:.1f}KB, ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            print(f"âš¡ Real-time factor: {duration/generation_time:.1f}x")
            
            return output_path, metadata
            
        except Exception as e:
            print(f"âŒ Advanced ZONOS TTS ìŒì„± ìƒì„± ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            if os.path.exists(output_path):
                os.unlink(output_path)
            raise e
    
    def generate_batch(
        self,
        texts: List[str],
        emotions: Optional[List[Union[str, List[float]]]] = None,
        speaker_names: Optional[List[str]] = None,
        cfg_scale: float = 2.5
    ) -> List[Tuple[str, AudioMetadata]]:
        """
        ë°°ì¹˜ ìŒì„± ìƒì„± - ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
        
        Args:
            texts: ìƒì„±í•  í…ìŠ¤íŠ¸ ëª©ë¡
            emotions: ê° í…ìŠ¤íŠ¸ë³„ ê°ì • (Noneì´ë©´ ëª¨ë‘ neutral)
            speaker_names: ê° í…ìŠ¤íŠ¸ë³„ í™”ìëª… (Noneì´ë©´ ëª¨ë‘ ê¸°ë³¸ í™”ì)
            cfg_scale: ìƒì„± í’ˆì§ˆ ì œì–´
            
        Returns:
            [(audio_file_path, metadata), ...] ëª©ë¡
        """
        print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        results = []
        total_start_time = time.time()
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if emotions is None:
            emotions = ["neutral"] * len(texts)
        if speaker_names is None:
            speaker_names = [None] * len(texts)
        
        # ê¸¸ì´ ë§ì¶”ê¸°
        emotions = emotions[:len(texts)] + ["neutral"] * (len(texts) - len(emotions))
        speaker_names = speaker_names[:len(texts)] + [None] * (len(texts) - len(speaker_names))
        
        for i, (text, emotion, speaker_name) in enumerate(zip(texts, emotions, speaker_names)):
            try:
                print(f"ğŸ”„ ë°°ì¹˜ {i+1}/{len(texts)} ì²˜ë¦¬ ì¤‘...")
                audio_path, metadata = self.generate_speech(
                    text=text,
                    emotion=emotion,
                    speaker_name=speaker_name,
                    cfg_scale=cfg_scale
                )
                results.append((audio_path, metadata))
                print(f"âœ… ë°°ì¹˜ {i+1}/{len(texts)} ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i+1}/{len(texts)} ì‹¤íŒ¨: {e}")
                results.append((None, None))
        
        total_time = time.time() - total_start_time
        successful_count = sum(1 for result in results if result[0] is not None)
        
        print(f"ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {successful_count}/{len(texts)} ì„±ê³µ, ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        return results
    
    def clear_cache(self):
        """Speaker embedding ìºì‹œ ì´ˆê¸°í™”"""
        self.embedding_cache.clear()